{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fastai+huggingface session 2: Using Transformers",
      "provenance": [],
      "collapsed_sections": [
        "-oB2-IzTddu-",
        "yuH-yEZxmPFC",
        "VDMcj3W6mXe7",
        "h_MUiijqm3yZ",
        "ySvl4jmGnC8Q",
        "Y9EOWyG4ni6g",
        "gIIT_ZiTni6k",
        "FoTgbdn3ni6n",
        "4TfE24HhtV54"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibha-rajan/check/blob/master/fastai%2Bhuggingface_session_2_Using_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oB2-IzTddu-"
      },
      "source": [
        "## Behind the pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u01PzKJXddvA"
      },
      "source": [
        "Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNIzlYPNddvA"
      },
      "source": [
        "! pip install -qq datasets transformers[sentencepiece]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBDKWtZeddvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce9735c9-f47a-4fae-8800-2b7f1b1fa42d"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier([\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
        "    \"I hate this so much!\",\n",
        "])\n",
        "print(classifier.model.name_or_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distilbert-base-uncased-finetuned-sst-2-english\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuH-yEZxmPFC"
      },
      "source": [
        "### Step 1: Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEEAmeWlmpbd"
      },
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWhIILtrddvB"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvhNdwsQddvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013e7fd7-376d-4b42-e0b0-fdb0ec248b14"
      },
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDMcj3W6mXe7"
      },
      "source": [
        "### Step 2: Run inputs through model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkdoS9l_ddvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd15662-eb05-43ca-8808-3023b4e818b4"
      },
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N32nQbhRealJ",
        "outputId": "3b0e886a-7c7f-4394-cd84-71f9643daf72"
      },
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 16, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwBRdgVAddvD"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OcCtGFvtUCP",
        "outputId": "aa489e5c-de08-48d9-e9bb-fd7871928687"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (1): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (2): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (3): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (4): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "        (5): TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlauCmCCddvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21be1828-3021-47bb-f94b-4e8da18c126c"
      },
      "source": [
        "print(outputs.logits.shape)\n",
        "print(outputs.logits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2])\n",
            "tensor([[-1.5607,  1.6123],\n",
            "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_MUiijqm3yZ"
      },
      "source": [
        "### Step 3: Process outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j8E0IUuddvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c3d270-1c08-4195-a802-4a45bcfacc1b"
      },
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.0195e-02, 9.5980e-01],\n",
            "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv2PFAltddvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902500f3-20db-4571-c4dc-26217b63a253"
      },
      "source": [
        "model.config.id2label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4gisEpyt_jj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySvl4jmGnC8Q"
      },
      "source": [
        "## Behind the pipeline (Blurr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9EOWyG4ni6g"
      },
      "source": [
        "### Step 1: Prepare inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVlJt847n5sf"
      },
      "source": [
        "!pip install -qq fastai\n",
        "!pip install -qq ohmeow-blurr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idyzUmfPoVIM"
      },
      "source": [
        "from fastai.text.all import * \n",
        "from blurr.utils import *\n",
        "from blurr.data.core import *\n",
        "from blurr.modeling.core import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE_iEvJyni6j"
      },
      "source": [
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "NlFz_DIIokHl",
        "outputId": "537373a4-6339-423a-9694-9f08d23816e0"
      },
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)\n",
        "imdb_df = pd.read_csv(path/'texts.csv')\n",
        "\n",
        "imdb_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label  ... is_valid\n",
              "0  negative  ...    False\n",
              "1  positive  ...    False\n",
              "2  negative  ...    False\n",
              "3  positive  ...    False\n",
              "4  negative  ...    False\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k797M-MHokEB",
        "outputId": "c1a69780-7c3d-457e-f968-8944e2c58616"
      },
      "source": [
        "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(checkpoint, model_cls=AutoModelForSequenceClassification)\n",
        "\n",
        "print(hf_arch)\n",
        "print(type(hf_config))\n",
        "print(type(hf_tokenizer))\n",
        "print(type(hf_model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distilbert\n",
            "<class 'transformers.models.distilbert.configuration_distilbert.DistilBertConfig'>\n",
            "<class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
            "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTt7iATzokAB"
      },
      "source": [
        "# single input\n",
        "blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=128, padding=True, truncation=True), CategoryBlock)\n",
        "dblock = DataBlock(blocks=blocks, get_x=ColReader('text'), get_y=ColReader('label'), splitter=ColSplitter())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6HgIruToj8e"
      },
      "source": [
        "dls = dblock.dataloaders(imdb_df, bs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "VEKHRtF4ni6j",
        "outputId": "121427fa-1cef-458f-a462-aae67fad63f3"
      },
      "source": [
        "dls.show_batch(dataloaders=dls, max_n=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>raising victor vargas : a review &lt; br / &gt; &lt; br / &gt; you know, raising victor vargas is like sticking your hands into a big, steaming bowl of oatmeal. it's warm and gooey, but you're not sure if it feels right. try as i might, no matter how warm and gooey raising victor vargas became i was always aware that something didn't quite feel right. victor vargas suffers from a certain overconfidence on the director's part. apparently, the director thought that the ethnic backdrop of a latino family on the lower east side, and an idyllic</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i had read many good things about this adaptation of my favorite novel... so invariably my expectations were crushed. but they were crushed more than should be expected. the movie would have been a decent movie if i had not read the novel beforehand, which perhaps ruined it for me. &lt; br / &gt; &lt; br / &gt; in any event, for some reason they changed the labor camp at toulon to a ship full of galley slaves. the scene at bishop myriel's was fine. in fact, other than the galleys, things survived up until the dismissal of fantine. because we don't want to have bad</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fbtaQGqni6j"
      },
      "source": [
        "xb, yb = dls.one_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPZRVozBpiqU",
        "outputId": "05109642-5978-4a3f-a6ee-2a66e79a1145"
      },
      "source": [
        "xb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
              " 'input_ids': tensor([[  101,  6274,  5125, 20556,  1024,  1037,  3319,  1026,  7987,  1013,\n",
              "           1028,  1026,  7987,  1013,  1028,  2017,  2113,  1010,  6274,  5125,\n",
              "          20556,  2003,  2066, 13423,  2115,  2398,  2046,  1037,  2502,  1010,\n",
              "          19986,  4605,  1997,  1051,  4017,  4168,  2389,  1012,  2009,  1005,\n",
              "           1055,  4010,  1998, 27571,  3240,  1010,  2021,  2017,  1005,  2128,\n",
              "           2025,  2469,  2065,  2009,  5683,  2157,  1012,  3046,  2004,  1045,\n",
              "           2453,  1010,  2053,  3043,  2129,  4010,  1998, 27571,  3240,  6274,\n",
              "           5125, 20556,  2150,  1045,  2001,  2467,  5204,  2008,  2242,  2134,\n",
              "           1005,  1056,  3243,  2514,  2157,  1012,  5125, 20556, 17567,  2013,\n",
              "           1037,  3056,  2058,  8663, 20740,  5897,  2006,  1996,  2472,  1005,\n",
              "           1055,  2112,  1012,  4593,  1010,  1996,  2472,  2245,  2008,  1996,\n",
              "           5636, 18876,  1997,  1037,  7402,  2155,  2006,  1996,  2896,  2264,\n",
              "           2217,  1010,  1998,  2019,  8909,  8516, 10415,   102],\n",
              "         [  101,  1996,  4497,  2105,  1996,  3420,  2003,  2028,  1997,  1996,\n",
              "           4086,  4355,  1998,  2087,  2514,  1011,  2204,  6298, 22092,  2412,\n",
              "           2081,  1012,  2045,  1005,  1055,  2074,  2053,  2893,  2105,  2008,\n",
              "           1010,  1998,  2009,  1005,  1055,  2524,  2000,  2941,  2404,  2028,\n",
              "           1005,  1055,  3110,  2005,  2023,  2143,  2046,  2616,  1012,  2009,\n",
              "           1005,  1055,  2025,  2028,  1997,  2216,  3152,  2008,  5363,  2205,\n",
              "           2524,  1010,  4496,  2515,  2009,  2272,  2039,  2007,  1996,  5976,\n",
              "           4355,  2825, 16820,  2000,  2131,  1996,  2048, 21989,  2362,  1999,\n",
              "           1996,  2203,  1012,  1999,  2755,  1010,  2035,  2049, 11084,  2003,\n",
              "          25605,  1010,  4838,  2306,  1996,  3494,  1998,  1996,  4292,  1998,\n",
              "           1996,  5436,  1012,  1012,  1012,  2029,  2003,  3811, 19337,  2666,\n",
              "          12423,  2000,  9573,  1012,  2009,  1005,  1055,  3733,  2000,  2228,\n",
              "           2008,  2107,  1037,  2293,  2466,  1010,  2004,   102],\n",
              "         [  101,  1045,  2453,  2025,  2022,  1037,  4121, 19837,  2099,  1997,\n",
              "           1996,  2434,  1000, 19815, 22231,  2860,  1000,  1010,  2021,  2049,\n",
              "          11669,  2100,  8297,  3084,  2008,  9637,  2298,  2066, 15401,   999,\n",
              "           1998,  2000,  2228,  1045,  2001,  2183,  2046,  2023,  8074,  2000,\n",
              "           2066,  2023,  2028,  2062,  1012,  2274,  2086,  2044,  2049,  8646,\n",
              "           1010,  2577, 18290,  4152,  2067,  2006,  2604,  1996, 14925,  5021,\n",
              "           2806,  4446,  1998,  2006,  2023, 26256, 25636,  1996,  9000,  2005,\n",
              "           3889,  2332,  1005,  1055,  2093,  3441,  1012,  2295,  1010,  1996,\n",
              "           3257,  2003,  4375,  2125,  2000,  2745,  2175,  6826,  6799,  1012,\n",
              "           1996,  2143,  3262,  6904, 21928,  2015,  1999,  2008,  7814,  2007,\n",
              "          17837, 22569,  2011,  2175,  6826,  6799,  1012,  2021,  2087,  1997,\n",
              "           1996,  7499,  2052,  2031,  2000,  2175,  2000, 18290,  1005,  1055,\n",
              "          21794,  2135, 10763,  1998,  4895,  4783,  5400,   102],\n",
              "         [  101,  5432,   999,  2023,  3319,  2097,  7487,  1996,  4566,  1997,\n",
              "           1996,  3185,  1010,  1000, 23348,  1012,  1000,  2065,  2017,  2123,\n",
              "           1005,  1056,  2215,  2000,  2113,  2129,  1996,  3185,  4515,  1010,\n",
              "           2123,  1005,  1056,  3191,  2023,  3319,   999,  1026,  7987,  1013,\n",
              "           1028,  1026,  7987,  1013,  1028,  1000, 23348,  1000,  2003,  2061,\n",
              "           2919,  2017,  1005,  2222,  2228,  1000,  8194,  2534,  1000,  2001,\n",
              "           1037, 19857,  3489,  1012,  1026,  7987,  1013,  1028,  1026,  7987,\n",
              "           1013,  1028,  2009,  4152,  2028,  2732,  2138,  2017,  2131,  2000,\n",
              "           2156,  6621,  2990,  2386,  1005,  1055,  6248,  3108,  1012,  2008,\n",
              "           1005,  1055,  1996,  2069,  2518,  1000, 23348,  1000,  2038,  2183,\n",
              "           2005,  2009,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
              "           1028, 13703,  5297,  1005,  1055, 28616, 15707,  4890,  1010,  1998,\n",
              "           2010,  8081,  3370,  2006,  2308,  2402,  2438,   102]],\n",
              "        device='cuda:0')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LcQ8K7Xpch1",
        "outputId": "7391f428-c602-42c3-a124-d7d13748486e"
      },
      "source": [
        "len(xb), xb['input_ids'].shape, xb['attention_mask'].shape, len(xb['input_ids']), yb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, torch.Size([4, 128]), torch.Size([4, 128]), 4, torch.Size([4]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIIT_ZiTni6k"
      },
      "source": [
        "### Step 2: Run inputs through model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfK74IR7ni6k"
      },
      "source": [
        "hf_model.cuda()\n",
        "outputs = hf_model(**xb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEXjih43ni6l",
        "outputId": "40895b3e-09ca-4253-8dab-3c7be0eeb2da"
      },
      "source": [
        "print(outputs.logits.shape)\n",
        "print(outputs.logits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2])\n",
            "tensor([[-1.0525,  1.2515],\n",
            "        [ 3.9893, -3.3104],\n",
            "        [ 0.1730, -0.0150],\n",
            "        [-1.4008,  1.4509]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoTgbdn3ni6n"
      },
      "source": [
        "### Step 3: Process outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpMXLGMpni6n",
        "outputId": "b87e9c1d-cc4c-4a9a-b4c2-3573261b9369"
      },
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[9.0788e-02, 9.0921e-01],\n",
            "        [9.9932e-01, 6.7533e-04],\n",
            "        [5.4686e-01, 4.5314e-01],\n",
            "        [5.4596e-02, 9.4540e-01]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow8ayR4Qni6n",
        "outputId": "b15d0a13-efe2-40aa-9b75-b62372cc3833"
      },
      "source": [
        "model.config.id2label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clwepMYWq2ee"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snleliD1rWKZ"
      },
      "source": [
        "### Bonus: Using Blurr Learner to look at training/validation results and for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fAvENVxq_SY"
      },
      "source": [
        "model = HF_BaseModelWrapper(hf_model)\n",
        "\n",
        "learn = Learner(dls, \n",
        "                model,\n",
        "                opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n",
        "                loss_func=CrossEntropyLossFlat(),\n",
        "                metrics=[accuracy],\n",
        "                cbs=[HF_BaseModelCallback],\n",
        "                splitter=hf_splitter)\n",
        "\n",
        "learn.freeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "cBy44HE4rAqT",
        "outputId": "78b24261-f896-4b15-889a-9f38e3704c8f"
      },
      "source": [
        "learn.show_results(learner=learn, max_n=2, trunc_at=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the trouble with the book, \" memoirs of a geisha \" is that it had japanese surfaces but underneath the surfaces it was all an american man's way of thinking. reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesso far from japanese ways of thinking were the characters. &lt; br / &gt; &lt; br / &gt; the movie isn't about japan or real geisha. it is a story about a few american men's mistaken ideas about japan an</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt; br / &gt; &lt; br / &gt; i'm sure things didn't exactly go the same way in the real life of homer hickam as they did in the film adaptation of his book, rocket boys, but the movie \" october sky \" ( an anagram of the book's title ) is good enough to stand alone. i have not read hickam's memoirs, but i am still able to enjoy and understand their film adaptation. the film, directed by joe johnston and written by lewis colick, records the story of teenager homer hickam ( jake gyllenhaal ), beginning in oct</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJyVgONurGdb",
        "outputId": "84d2d56c-86ab-496c-8e22-6980f8a46551"
      },
      "source": [
        "learn.blurr_predict([\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
        "    \"I hate this so much!\",\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('positive',), (#1) [tensor(1)], (#1) [tensor([0.0402, 0.9598])]),\n",
              " (('negative',), (#1) [tensor(0)], (#1) [tensor([9.9946e-01, 5.4418e-04])])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFlIg4CEtVLd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TfE24HhtV54"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEygl9_DrwbO"
      },
      "source": [
        "!mkdir -p 'my_model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tHqnWz7rrM7",
        "outputId": "4c00a1f7-6c30-4c4f-e219-a2c108d23051"
      },
      "source": [
        "learn.model.hf_model.save_pretrained('my_model')\n",
        "hf_tokenizer.save_pretrained('my_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('my_model/tokenizer_config.json',\n",
              " 'my_model/special_tokens_map.json',\n",
              " 'my_model/vocab.txt',\n",
              " 'my_model/added_tokens.json',\n",
              " 'my_model/tokenizer.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSUDpdYTGSK",
        "outputId": "cd51e7fb-de96-4b6c-805d-146eaf99e148"
      },
      "source": [
        "hf_model is learn.model.hf_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNO3CxYwr77I",
        "outputId": "aff6fda7-b617-4877-83d1-1127684524a9"
      },
      "source": [
        "!ls -lsha 'my_model'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 257M\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jul 18 15:44 .\n",
            "4.0K drwxr-xr-x 1 root root 4.0K Jul 18 15:44 ..\n",
            "4.0K -rw-r--r-- 1 root root  734 Jul 18 18:03 config.json\n",
            "256M -rw-r--r-- 1 root root 256M Jul 18 18:03 pytorch_model.bin\n",
            "4.0K -rw-r--r-- 1 root root  112 Jul 18 18:03 special_tokens_map.json\n",
            "4.0K -rw-r--r-- 1 root root  405 Jul 18 18:03 tokenizer_config.json\n",
            "456K -rw-r--r-- 1 root root 456K Jul 18 18:03 tokenizer.json\n",
            "228K -rw-r--r-- 1 root root 227K Jul 18 18:03 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjbXgV5Br_Z5",
        "outputId": "eb17ae30-2bbd-4aa0-c80b-9cdf5cf233c5"
      },
      "source": [
        "hf_arch2, hf_config2, hf_tokenizer2, hf_model2 = BLURR.get_hf_objects('my_model', model_cls=AutoModelForSequenceClassification)\n",
        "\n",
        "print(hf_arch)\n",
        "print(type(hf_config))\n",
        "print(type(hf_tokenizer))\n",
        "print(type(hf_model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distilbert\n",
            "<class 'transformers.models.distilbert.configuration_distilbert.DistilBertConfig'>\n",
            "<class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
            "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzjak671sX9U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}